{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15c7235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32de2a2dfb54c358c3ede81c2758e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247441\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import common_util\n",
    "from tqdm import notebook\n",
    "\n",
    "splited_sentences = []\n",
    "\n",
    "BASE_FOLDER = r\"E:\\DATA\\copus\\NIKL_NEWSPAPER_v2.0\\data_kss\"\n",
    "\n",
    "all_lines = []\n",
    "folders = os.listdir(BASE_FOLDER)\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(BASE_FOLDER, folder)\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    files = notebook.tqdm(files)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        lines = common_util.read_lines(file_path)\n",
    "        # print(len(lines))\n",
    "        all_lines.extend(lines)\n",
    "\n",
    "print(len(all_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405ce145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# results = splited_sentences\n",
    "results = all_lines\n",
    "test_size = int(len(results) * 0.1)\n",
    "train, val = train_test_split(results, test_size=test_size, random_state=111)\n",
    "train, test = train_test_split(train, test_size=test_size, random_state=111)\n",
    "\n",
    "\n",
    "def write_dataset(file_name, dataset):\n",
    "    with open(\n",
    "        os.path.join(\"data\", file_name), mode=\"w\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        for data in dataset:\n",
    "            f.write(data)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "write_dataset(\"train.txt\", train)\n",
    "write_dataset(\"val.txt\", val)\n",
    "write_dataset(\"test.txt\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557c0976",
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (1957826071.py, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [7]\u001b[1;36m\u001b[0m\n\u001b[1;33m    sentence = [s for s in sentence]\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, data_path: str):\n",
    "        self.sentences = []\n",
    "        self.slot_labels = [\"UNK\", \"PAD\", \"B\", \"I\"]\n",
    "\n",
    "        self._load_data(data_path)\n",
    "\n",
    "    def _load_data(self, data_path: str):\n",
    "        \"\"\"data를 file에서 불러온다.\n",
    "\n",
    "        Args:\n",
    "            data_path: file 경로\n",
    "        \"\"\"\n",
    "        with open(data_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            self.sentences = [line.split() for line in lines]\n",
    "\n",
    "    def _get_tags(self, sentence: List[str]) -> List[str]:\n",
    "        \"\"\"문장에 대해 띄어쓰기 tagging을 한다.\n",
    "        character 단위로 분리하여 BI tagging을 한다.\n",
    "\n",
    "        Args:\n",
    "            sentence: 문장\n",
    "\n",
    "        Retrns:\n",
    "            문장의 각 토큰에 대해 tagging한 결과 리턴\n",
    "            [\"B\", \"I\"]\n",
    "        \"\"\"\n",
    "\n",
    "        tags = []\n",
    "        for word in sentence:\n",
    "            for i in range(len(word)):\n",
    "                if i == 0:\n",
    "                    tags.append(\"B\")\n",
    "                else:\n",
    "                    tags.append(\"I\")\n",
    "        return tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = \"\".join(self.sentences[idx])\n",
    "        sentence = [s for s in sentence]\n",
    "        tags = self._get_tags(self.sentences[idx])\n",
    "        tags = [self.slot_labels.index(t) for t in tags]\n",
    "\n",
    "        (\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            token_type_ids,\n",
    "            slot_label_ids, \n",
    "        ) = self.transform(sentence, tags)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids, slot_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6422374e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799a512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert",
   "language": "python",
   "name": "kobert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
